{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trace_extract import parse_trace_file, get_directed\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "#device = torch.device(2 if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN_GRU, self).__init__();\n",
    "        self.gru = nn.GRU(1, 8, 2)\n",
    "        self.linear = nn.Linear(8, 1)\n",
    "        self.hidden_state = torch.randn(2, 1, 8, requires_grad=False)\n",
    "        \n",
    "    def forward(self, x):#(1, 1, 1) seq_len, batch_size, input_size\n",
    "        self.hidden_state= self.hidden_state.to(device)\n",
    "        x, self.hidden_state=self.gru(x, self.hidden_state)#(1, 1, 8)\n",
    "        #self.hidden_state = self.hidden_state.detach()\n",
    "        x=x.view(-1);\n",
    "        x=self.linear(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asymetric_loss(output, target, alpha=0.75):\n",
    "    if (output<target):\n",
    "        loss = 2.0*alpha*((output - target)**2)\n",
    "    else:\n",
    "        loss = 2.0*(1-alpha)*((output - target)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_monitor():\n",
    "    def __init__(self, self_id, monitoring_id, history_size=16):\n",
    "        self.ita=100000000;\n",
    "        self.id = self_id\n",
    "        self.monitoring = monitoring_id\n",
    "        self.history_size = history_size\n",
    "        self.arrival_history = deque(maxlen=history_size)\n",
    "        #self.margin_history = np.zeros(16);\n",
    "        self.expected_arrival = 0\n",
    "        self.safety_margin = Variable(torch.tensor([0]).float()).to(device)\n",
    "        self.rnn_module = RNN_GRU().to(device);\n",
    "        self.seq_num = -1;\n",
    "        self.suspect_intervals=[]\n",
    "        self.U = 0;\n",
    "        self.lr=1e-3\n",
    "        self.decay=1e-6\n",
    "        self.optimizer=optim.SGD(self.rnn_module.parameters(),\n",
    "                             lr=self.lr,\n",
    "                             weight_decay=self.decay)\n",
    "        self.criterion = asymetric_loss#nn.MSELoss()\n",
    "        self.loss = 0;\n",
    "    \n",
    "    def forward(self, seq_num, arrival_time):\n",
    "        if (self.seq_num == -1):\n",
    "            self.seq_num=seq_num\n",
    "            self.U=arrival_time\n",
    "            self.arrival_history.append(arrival_time)\n",
    "            self.expected_arrival = self.U + self.ita;\n",
    "            x=Variable(torch.tensor([0]).float()).to(device)\n",
    "            self.safety_margin = self.rnn_module.forward(x.view(1,1,1))\n",
    "        elif (self.seq_num+1==seq_num):\n",
    "            start_time=time.time();\n",
    "            self.rnn_module.zero_grad()\n",
    "            self.seq_num=seq_num\n",
    "            if (self.expected_arrival + self.safety_margin.item()*self.ita < arrival_time):\n",
    "                self.suspect_intervals.append([self.expected_arrival + self.safety_margin, arrival_time])\n",
    "            self.arrival_history.append(arrival_time)\n",
    "            #print(\"append arrival:\", time.time()-start_time);start_time=time.time();\n",
    "            optimal_margin = (arrival_time-self.expected_arrival)/self.ita\n",
    "            #self.margin_history=np.roll(self.margin_history, -1);\n",
    "            #self.margin_history[-1]=optimal_margin\n",
    "            target = Variable(torch.tensor([optimal_margin]).float()).to(device)\n",
    "            #print(\"compute optimal:\", time.time()-start_time);start_time=time.time();\n",
    "            self.optimizer.zero_grad()\n",
    "            self.loss= self.criterion(self.safety_margin, target)\n",
    "            #print(\"compute loss:\", time.time()-start_time);start_time=time.time();\n",
    "            self.loss.backward()\n",
    "            self.rnn_module.hidden_state = self.rnn_module.hidden_state.detach()\n",
    "            self.optimizer.step()\n",
    "            #print(\"back prop:\", time.time()-start_time);start_time=time.time();\n",
    "            input_x = Variable(torch.tensor([optimal_margin]).float()).to(device)            \n",
    "            self.safety_margin = self.rnn_module.forward(input_x.view(1,1,1))\n",
    "            #print(\"estimate next:\", time.time()-start_time);start_time=time.time();\n",
    "            self.cal_expectation()\n",
    "            \n",
    "        elif (self.seq_num<seq_num):\n",
    "            self.seq_num=seq_num\n",
    "            self.arrival_history.clear();\n",
    "            self.U=arrival_time\n",
    "            self.arrival_history.append(arrival_time)\n",
    "            self.expected_arrival = self.U + self.ita;\n",
    "    \n",
    "    def cal_expectation(self):\n",
    "        if (len(self.arrival_history)<self.history_size):\n",
    "            k = len(self.arrival_history);\n",
    "            temp_U = self.arrival_history[-1]/(k)+(k-1)*self.U/(k)\n",
    "            self.U = temp_U\n",
    "            self.expected_arrival = self.U + (k+1)/2*self.ita;\n",
    "        else:\n",
    "            temp = (self.arrival_history[-1]-self.arrival_history[0])/(self.history_size-1)\n",
    "            self.expected_arrival += temp\n",
    "        return self.expected_arrival\n",
    "\n",
    "    def combine_hidden_state(self, other_state, alpha=0.9):\n",
    "        self.rnn_module.hidden_state=alpha*self.rnn_module.hidden_state+(1-alpha)*other_state\n",
    "        self.rnn_module.hidden_state=self.rnn_module.hidden_state.detach();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traces(trace_list):\n",
    "    traces=[]\n",
    "    lens=[]\n",
    "    for p in trace_list:\n",
    "        u = p[0]; v = p[1];\n",
    "        temp_t, temp_l = get_directed(u, v);\n",
    "        traces.append(temp_t)\n",
    "        lens.append(temp_l)\n",
    "    return traces, lens\n",
    "\n",
    "def create_monitors(trace_list, monitor_type):\n",
    "    monitors=[]\n",
    "    for p in trace_list:\n",
    "        u = p[0]; v = p[1];\n",
    "        monitors.append(monitor_type(u, v))\n",
    "    return monitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_list = [(1, 0), (3, 0), (4, 0)]\n",
    "num_traces = len(trace_list)\n",
    "traces, lens = get_traces(trace_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monitor from 1 to 0:\n",
      "expected and safety margin 47816598.968825184 0.00048332139849662783\n",
      "maximum waiting time 47816598.9693085\n",
      "actual time 47816598.96817092\n",
      "RNN loss tensor([6.2920e-05], grad_fn=<MulBackward0>)\n",
      "monitor from 1 to 0:\n",
      "expected and safety margin 47817098.96987743 0.0007333122193813324\n",
      "maximum waiting time 47817098.97061074\n",
      "actual time 47817098.969323605\n",
      "RNN loss tensor([0.0050], grad_fn=<MulBackward0>)\n",
      "monitor from 1 to 0:\n",
      "expected and safety margin 47817598.96838807 0.0008244499564170838\n",
      "maximum waiting time 47817598.969212525\n",
      "actual time 47817598.96763435\n",
      "RNN loss tensor([5.1011e-05], grad_fn=<MulBackward0>)\n",
      "monitor from 1 to 0:\n",
      "expected and safety margin 47818098.96855063 0.0005854599177837372\n",
      "maximum waiting time 47818098.96913609\n",
      "actual time 47818098.96902764\n",
      "RNN loss tensor([6.2924e-06], grad_fn=<MulBackward0>)\n",
      "monitor from 3 to 0:\n",
      "expected and safety margin 1258014.6475778127 0.0005475431680679321\n",
      "maximum waiting time 1258014.6481253558\n",
      "actual time 1258014.646732207\n",
      "RNN loss tensor([7.1040e-05], grad_fn=<MulBackward0>)\n",
      "monitor from 3 to 0:\n",
      "expected and safety margin 1258514.648441123 0.000837370753288269\n",
      "maximum waiting time 1258514.6492784938\n",
      "actual time 1258514.647790931\n",
      "RNN loss tensor([0.0049], grad_fn=<MulBackward0>)\n",
      "monitor from 3 to 0:\n",
      "expected and safety margin 1259014.6490772374 0.0015088170766830444\n",
      "maximum waiting time 1259014.6505860544\n",
      "actual time 1259014.646218481\n",
      "RNN loss tensor([0.0008], grad_fn=<MulBackward0>)\n",
      "monitor from 3 to 0:\n",
      "expected and safety margin 1259514.6473661147 0.0005403012037277222\n",
      "maximum waiting time 1259514.6479064159\n",
      "actual time 1259514.647728769\n",
      "RNN loss tensor([4.2094e-06], grad_fn=<MulBackward0>)\n",
      "monitor from 4 to 0:\n",
      "expected and safety margin 9301920.748911882 0.0007566168904304504\n",
      "maximum waiting time 9301920.7496685\n",
      "actual time 9301920.748021875\n",
      "RNN loss tensor([6.8502e-05], grad_fn=<MulBackward0>)\n",
      "monitor from 4 to 0:\n",
      "expected and safety margin 9302420.75049509 0.0009832561016082763\n",
      "maximum waiting time 9302420.751478346\n",
      "actual time 9302420.749401111\n",
      "RNN loss tensor([0.0106], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "error_history = []\n",
    "for i in range(num_traces):\n",
    "    error_history.append([])\n",
    "computation_times = []\n",
    "for i in range(num_traces):\n",
    "    computation_times.append([])\n",
    "start_time = []\n",
    "end_time = [0 for i in range(num_traces)]\n",
    "monitors = create_monitors(trace_list, RNN_monitor)\n",
    "for i in range(num_traces):\n",
    "    count=0;\n",
    "    for j in range(len(traces[i])):\n",
    "        n = traces[i][j][0]\n",
    "        t = traces[i][j][1]\n",
    "        if count==0:\n",
    "            start_time.append(t)\n",
    "        end_time[i] = t;\n",
    "        expt=monitors[i].expected_arrival/1e9\n",
    "        sft=monitors[i].safety_margin.item()/10\n",
    "        maxt=expt+sft\n",
    "        error_history[i].append(t/1e9-maxt)\n",
    "        if (n%5000==1):\n",
    "            print(\"monitor from %d to %d:\"%(trace_list[i][0], trace_list[i][1]))\n",
    "            print(\"expected and safety margin\", expt, sft)\n",
    "            print(\"maximum waiting time\", maxt)\n",
    "            print(\"actual time\", t/1e9)\n",
    "            print(\"RNN loss\", monitors[i].loss)\n",
    "        timestamp=time.time();\n",
    "        monitors[i].forward(n, t)\n",
    "        computation_times[i].append(time.time()-timestamp);\n",
    "    count+=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (46) : all CUDA-capable devices are busy or unavailable at /opt/conda/conda-bld/pytorch_1544174967633/work/torch/csrc/generic/serialization.cpp:15",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7acbd3dab302>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../output_data/CNN_suspect%d_%d.pkl\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuspect_intervals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrival_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrival_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/storage.py\u001b[0m in \u001b[0;36m__reduce__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_load_from_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0mserialized_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_should_read_directly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (46) : all CUDA-capable devices are busy or unavailable at /opt/conda/conda-bld/pytorch_1544174967633/work/torch/csrc/generic/serialization.cpp:15"
     ]
    }
   ],
   "source": [
    "print(start_time)\n",
    "print(end_time)\n",
    "for i in range(num_traces):\n",
    "    u = trace_list[i][0]; v = trace_list[i][1]\n",
    "    f=open(\"../output_data/CoRNN_suspect%d_%d.pkl\"%(u, v), \"wb\");\n",
    "    pkl.dump(monitors[i].suspect_intervals, f)\n",
    "    pkl.dump(start_time[i], f)\n",
    "    pkl.dump(end_time[i], f)\n",
    "    pkl.dump(error_history[i], f)\n",
    "    pkl.dump(computation_times[i], f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_traces):\n",
    "    plt.plot(error_history[i][1:])\n",
    "    plt.ylim(-0.1, 0.1)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
